{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实例演示使用MedLvmWorkflow训练resnet50模型，MedLvmWorkflow默认提供常用的开源医疗数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "首先，从 opendatalab 下载 Camvid 数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# https://opendatalab.com/CamVid\n",
    "\n",
    "  pip install openxlab #安装\n",
    "\n",
    "  pip install -U openxlab #版本升级\n",
    "  \n",
    "  openxlab login #进行登录，输入对应的AK/SK\n",
    "  \n",
    "  openxlab dataset info --dataset-repo OpenDataLab/CamVid #数据集信息查看\n",
    "\n",
    "  openxlab dataset ls --dataset-repo OpenDataLab/CamVid #数据集文件列表查看\n",
    "\n",
    "  openxlab dataset get --dataset-repo OpenDataLab/CamVid #数据集下载\n",
    "  \n",
    "  openxlab dataset download --dataset-repo OpenDataLab/CamVid --source-path /README.md --target-path /path/to/local/folder #数据集文件下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from MedLvmWorkflow.dataset.segmentation.demoDataset import create_palette\n",
    "from MedLvmWorkflow.models.modelFactory import createTrainModel\n",
    "color_to_class=create_palette(\"../data/CamVid/class_dict.csv\")\n",
    "num_class=len(color_to_class.items())\n",
    "trainModel =createTrainModel(\"deeplabv3_resnet50_pretrained\",num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import MedLvmWorkflow\n",
    "from MedLvmWorkflow.dataset.segmentation.demoDataset import CamVid,create_palette\n",
    "\n",
    "train_set = CamVid(\n",
    "    '../data/CamVid',\n",
    "    img_folder='train',\n",
    "    mask_folder='train_labels',\n",
    "    transform=trainModel.transform_img,\n",
    "    target_transform=trainModel.transform_lable)\n",
    "\n",
    "valid_set = CamVid(\n",
    "    '../data/CamVid',\n",
    "    img_folder='val',\n",
    "    mask_folder='val_labels',\n",
    "    transform=trainModel.transform_img,\n",
    "    target_transform=trainModel.transform_lable)\n",
    "# print(color_to_class)\n",
    "# print(num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/14 13:18:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 15160592\n",
      "    GPU 0: NVIDIA GeForce RTX 2080 Ti\n",
      "    CUDA_HOME: /usr/local/cuda-11.6\n",
      "    NVCC: Cuda compilation tools, release 11.6, V11.6.55\n",
      "    GCC: gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0\n",
      "    PyTorch: 2.1.2+cu118\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.16.2+cu118\n",
      "    OpenCV: 4.9.0\n",
      "    MMEngine: 0.10.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 15160592\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/14 13:18:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/14 13:18:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SegmentationVisHook                \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "06/14 13:18:29 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CamVid has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "06/14 13:18:29 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CamVid has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "06/14 13:18:29 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/14 13:18:29 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "06/14 13:18:29 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/botao/liangjun/LVMWorkFlow/notebooks/trainlogs/train_deeplabv3_resnet50.\n",
      "06/14 13:18:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][10/92]  lr: 2.0000e-04  eta: 0:21:09  time: 0.6937  data_time: 0.1396  memory: 6874  loss: 2.0635\n",
      "06/14 13:18:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][20/92]  lr: 2.0000e-04  eta: 0:20:33  time: 0.6778  data_time: 0.1707  memory: 6874  loss: 1.6077\n",
      "06/14 13:18:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][30/92]  lr: 2.0000e-04  eta: 0:19:32  time: 0.6478  data_time: 0.1559  memory: 6874  loss: 1.3887\n",
      "06/14 13:18:54 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][40/92]  lr: 2.0000e-04  eta: 0:18:59  time: 0.6329  data_time: 0.1486  memory: 6874  loss: 1.2632\n",
      "06/14 13:19:00 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][50/92]  lr: 2.0000e-04  eta: 0:18:37  time: 0.6245  data_time: 0.1444  memory: 6874  loss: 1.1694\n",
      "06/14 13:19:06 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][60/92]  lr: 2.0000e-04  eta: 0:18:21  time: 0.6187  data_time: 0.1414  memory: 6874  loss: 1.1054\n",
      "06/14 13:19:12 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][70/92]  lr: 2.0000e-04  eta: 0:18:07  time: 0.6146  data_time: 0.1393  memory: 6874  loss: 1.0491\n",
      "06/14 13:19:18 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][80/92]  lr: 2.0000e-04  eta: 0:17:56  time: 0.6116  data_time: 0.1376  memory: 6874  loss: 0.9984\n",
      "06/14 13:19:24 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][90/92]  lr: 2.0000e-04  eta: 0:17:46  time: 0.6095  data_time: 0.1364  memory: 6874  loss: 0.9579\n",
      "06/14 13:19:25 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:20:12 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [1][10/25]    eta: 0:01:09  time: 4.6431  data_time: 4.2976  memory: 6874  \n",
      "06/14 13:20:16 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [1][20/25]    eta: 0:00:12  time: 2.5554  data_time: 2.2124  memory: 1696  \n",
      "06/14 13:20:19 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][25/25]    t/iou: 0.8631  data_time: 1.7952  time: 2.1380\n",
      "06/14 13:20:25 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][10/92]  lr: 2.0000e-04  eta: 0:17:35  time: 0.5981  data_time: 0.1349  memory: 6874  loss: 0.8651\n",
      "06/14 13:20:31 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][20/92]  lr: 2.0000e-04  eta: 0:17:27  time: 0.5907  data_time: 0.1266  memory: 6874  loss: 0.7518\n",
      "06/14 13:20:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][30/92]  lr: 2.0000e-04  eta: 0:17:20  time: 0.5913  data_time: 0.1264  memory: 6874  loss: 0.6950\n",
      "06/14 13:20:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][40/92]  lr: 2.0000e-04  eta: 0:17:12  time: 0.5922  data_time: 0.1264  memory: 6874  loss: 0.6496\n",
      "06/14 13:20:48 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][50/92]  lr: 2.0000e-04  eta: 0:17:06  time: 0.5932  data_time: 0.1264  memory: 6874  loss: 0.6098\n",
      "06/14 13:20:54 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][60/92]  lr: 2.0000e-04  eta: 0:16:59  time: 0.5943  data_time: 0.1265  memory: 6874  loss: 0.5784\n",
      "06/14 13:21:00 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][70/92]  lr: 2.0000e-04  eta: 0:16:53  time: 0.5953  data_time: 0.1264  memory: 6874  loss: 0.5450\n",
      "06/14 13:21:06 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][80/92]  lr: 2.0000e-04  eta: 0:16:47  time: 0.5968  data_time: 0.1264  memory: 6874  loss: 0.5172\n",
      "06/14 13:21:13 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [2][90/92]  lr: 2.0000e-04  eta: 0:16:41  time: 0.5984  data_time: 0.1266  memory: 6874  loss: 0.4960\n",
      "06/14 13:21:14 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:22:00 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [2][10/25]    eta: 0:01:09  time: 2.8496  data_time: 2.5043  memory: 6874  \n",
      "06/14 13:22:05 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [2][20/25]    eta: 0:00:12  time: 2.3213  data_time: 1.9758  memory: 1696  \n",
      "06/14 13:22:07 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][25/25]    t/iou: 0.8830  data_time: 1.7233  time: 2.0715\n",
      "06/14 13:22:13 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][10/92]  lr: 2.0000e-04  eta: 0:16:34  time: 0.6002  data_time: 0.1269  memory: 6874  loss: 0.4650\n",
      "06/14 13:22:19 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][20/92]  lr: 2.0000e-04  eta: 0:16:28  time: 0.6015  data_time: 0.1272  memory: 6874  loss: 0.4484\n",
      "06/14 13:22:25 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][30/92]  lr: 2.0000e-04  eta: 0:16:23  time: 0.6032  data_time: 0.1272  memory: 6874  loss: 0.4360\n",
      "06/14 13:22:32 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][40/92]  lr: 2.0000e-04  eta: 0:16:18  time: 0.6052  data_time: 0.1274  memory: 6874  loss: 0.4211\n",
      "06/14 13:22:38 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][50/92]  lr: 2.0000e-04  eta: 0:16:12  time: 0.6071  data_time: 0.1275  memory: 6874  loss: 0.4078\n",
      "06/14 13:22:44 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][60/92]  lr: 2.0000e-04  eta: 0:16:07  time: 0.6095  data_time: 0.1275  memory: 6874  loss: 0.3949\n",
      "06/14 13:22:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][70/92]  lr: 2.0000e-04  eta: 0:16:03  time: 0.6123  data_time: 0.1272  memory: 6874  loss: 0.3846\n",
      "06/14 13:22:57 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][80/92]  lr: 2.0000e-04  eta: 0:15:58  time: 0.6158  data_time: 0.1273  memory: 6874  loss: 0.3752\n",
      "06/14 13:23:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [3][90/92]  lr: 2.0000e-04  eta: 0:15:54  time: 0.6193  data_time: 0.1272  memory: 6874  loss: 0.3669\n",
      "06/14 13:23:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:23:51 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [3][10/25]    eta: 0:01:09  time: 2.5560  data_time: 2.2088  memory: 6874  \n",
      "06/14 13:23:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [3][20/25]    eta: 0:00:12  time: 2.2608  data_time: 1.9122  memory: 1696  \n",
      "06/14 13:23:58 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][25/25]    t/iou: 0.8950  data_time: 1.7360  time: 2.0922\n",
      "06/14 13:24:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][10/92]  lr: 2.0000e-04  eta: 0:15:48  time: 0.6216  data_time: 0.1282  memory: 6874  loss: 0.3501\n",
      "06/14 13:24:11 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][20/92]  lr: 2.0000e-04  eta: 0:15:43  time: 0.6245  data_time: 0.1291  memory: 6874  loss: 0.3421\n",
      "06/14 13:24:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][30/92]  lr: 2.0000e-04  eta: 0:15:39  time: 0.6280  data_time: 0.1301  memory: 6874  loss: 0.3260\n",
      "06/14 13:24:24 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][40/92]  lr: 2.0000e-04  eta: 0:15:34  time: 0.6318  data_time: 0.1308  memory: 6874  loss: 0.3125\n",
      "06/14 13:24:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][50/92]  lr: 2.0000e-04  eta: 0:15:30  time: 0.6348  data_time: 0.1313  memory: 6874  loss: 0.3016\n",
      "06/14 13:24:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][60/92]  lr: 2.0000e-04  eta: 0:15:25  time: 0.6385  data_time: 0.1314  memory: 6874  loss: 0.2989\n",
      "06/14 13:24:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][70/92]  lr: 2.0000e-04  eta: 0:15:21  time: 0.6413  data_time: 0.1315  memory: 6874  loss: 0.2991\n",
      "06/14 13:24:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][80/92]  lr: 2.0000e-04  eta: 0:15:16  time: 0.6439  data_time: 0.1315  memory: 6874  loss: 0.2943\n",
      "06/14 13:24:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [4][90/92]  lr: 2.0000e-04  eta: 0:15:12  time: 0.6455  data_time: 0.1314  memory: 6874  loss: 0.2887\n",
      "06/14 13:24:58 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:25:44 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [4][10/25]    eta: 0:01:09  time: 2.4397  data_time: 2.0888  memory: 6874  \n",
      "06/14 13:25:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [4][20/25]    eta: 0:00:12  time: 2.2342  data_time: 1.8823  memory: 1696  \n",
      "06/14 13:25:52 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][25/25]    t/iou: 0.9001  data_time: 1.7324  time: 2.0946\n",
      "06/14 13:25:58 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][10/92]  lr: 2.0000e-04  eta: 0:15:04  time: 0.6441  data_time: 0.1312  memory: 6874  loss: 0.2847\n",
      "06/14 13:26:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][20/92]  lr: 2.0000e-04  eta: 0:14:59  time: 0.6443  data_time: 0.1300  memory: 6874  loss: 0.2808\n",
      "06/14 13:26:10 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][30/92]  lr: 2.0000e-04  eta: 0:14:53  time: 0.6441  data_time: 0.1290  memory: 6874  loss: 0.2804\n",
      "06/14 13:26:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][40/92]  lr: 2.0000e-04  eta: 0:14:48  time: 0.6446  data_time: 0.1279  memory: 6874  loss: 0.2768\n",
      "06/14 13:26:23 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][50/92]  lr: 2.0000e-04  eta: 0:14:43  time: 0.6445  data_time: 0.1280  memory: 6874  loss: 0.2765\n",
      "06/14 13:26:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][60/92]  lr: 2.0000e-04  eta: 0:14:37  time: 0.6449  data_time: 0.1278  memory: 6874  loss: 0.2723\n",
      "06/14 13:26:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][70/92]  lr: 2.0000e-04  eta: 0:14:32  time: 0.6446  data_time: 0.1279  memory: 6874  loss: 0.2650\n",
      "06/14 13:26:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][80/92]  lr: 2.0000e-04  eta: 0:14:27  time: 0.6441  data_time: 0.1278  memory: 6874  loss: 0.2574\n",
      "06/14 13:26:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [5][90/92]  lr: 2.0000e-04  eta: 0:14:21  time: 0.6433  data_time: 0.1278  memory: 6874  loss: 0.2472\n",
      "06/14 13:26:51 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:27:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [5][10/25]    eta: 0:01:10  time: 2.1496  data_time: 1.7962  memory: 6874  \n",
      "06/14 13:27:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [5][20/25]    eta: 0:00:12  time: 2.1513  data_time: 1.7967  memory: 1696  \n",
      "06/14 13:27:45 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][25/25]    t/iou: 0.9060  data_time: 1.7394  time: 2.0946\n",
      "06/14 13:27:51 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][10/92]  lr: 2.0000e-04  eta: 0:14:14  time: 0.6399  data_time: 0.1279  memory: 6874  loss: 0.2371\n",
      "06/14 13:27:57 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][20/92]  lr: 2.0000e-04  eta: 0:14:07  time: 0.6392  data_time: 0.1279  memory: 6874  loss: 0.2298\n",
      "06/14 13:28:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][30/92]  lr: 2.0000e-04  eta: 0:14:01  time: 0.6391  data_time: 0.1279  memory: 6874  loss: 0.2230\n",
      "06/14 13:28:10 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][40/92]  lr: 2.0000e-04  eta: 0:13:55  time: 0.6391  data_time: 0.1277  memory: 6874  loss: 0.2210\n",
      "06/14 13:28:16 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][50/92]  lr: 2.0000e-04  eta: 0:13:49  time: 0.6383  data_time: 0.1277  memory: 6874  loss: 0.2207\n",
      "06/14 13:28:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][60/92]  lr: 2.0000e-04  eta: 0:13:44  time: 0.6374  data_time: 0.1269  memory: 6874  loss: 0.2169\n",
      "06/14 13:28:29 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][70/92]  lr: 2.0000e-04  eta: 0:13:38  time: 0.6376  data_time: 0.1266  memory: 6874  loss: 0.2103\n",
      "06/14 13:28:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][80/92]  lr: 2.0000e-04  eta: 0:13:32  time: 0.6379  data_time: 0.1265  memory: 6874  loss: 0.2066\n",
      "06/14 13:28:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [6][90/92]  lr: 2.0000e-04  eta: 0:13:27  time: 0.6378  data_time: 0.1265  memory: 6874  loss: 0.2051\n",
      "06/14 13:28:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:29:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [6][10/25]    eta: 0:01:09  time: 2.1554  data_time: 1.7987  memory: 6874  \n",
      "06/14 13:29:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [6][20/25]    eta: 0:00:12  time: 2.1572  data_time: 1.7995  memory: 1696  \n",
      "06/14 13:29:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [6][25/25]    t/iou: 0.9117  data_time: 1.7350  time: 2.0947\n",
      "06/14 13:29:44 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][10/92]  lr: 2.0000e-04  eta: 0:13:19  time: 0.6363  data_time: 0.1275  memory: 6874  loss: 0.2019\n",
      "06/14 13:29:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][20/92]  lr: 2.0000e-04  eta: 0:13:13  time: 0.6380  data_time: 0.1283  memory: 6874  loss: 0.1986\n",
      "06/14 13:29:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][30/92]  lr: 2.0000e-04  eta: 0:13:07  time: 0.6398  data_time: 0.1291  memory: 6874  loss: 0.1981\n",
      "06/14 13:30:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][40/92]  lr: 2.0000e-04  eta: 0:13:01  time: 0.6411  data_time: 0.1298  memory: 6874  loss: 0.2008\n",
      "06/14 13:30:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][50/92]  lr: 2.0000e-04  eta: 0:12:55  time: 0.6427  data_time: 0.1300  memory: 6874  loss: 0.1990\n",
      "06/14 13:30:16 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][60/92]  lr: 2.0000e-04  eta: 0:12:50  time: 0.6445  data_time: 0.1298  memory: 6874  loss: 0.1987\n",
      "06/14 13:30:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][70/92]  lr: 2.0000e-04  eta: 0:12:44  time: 0.6448  data_time: 0.1299  memory: 6874  loss: 0.1977\n",
      "06/14 13:30:29 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][80/92]  lr: 2.0000e-04  eta: 0:12:38  time: 0.6450  data_time: 0.1299  memory: 6874  loss: 0.1946\n",
      "06/14 13:30:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [7][90/92]  lr: 2.0000e-04  eta: 0:12:33  time: 0.6458  data_time: 0.1302  memory: 6874  loss: 0.1912\n",
      "06/14 13:30:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:31:23 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [7][10/25]    eta: 0:01:09  time: 2.1580  data_time: 1.7992  memory: 6874  \n",
      "06/14 13:31:28 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [7][20/25]    eta: 0:00:12  time: 2.1573  data_time: 1.7986  memory: 1696  \n",
      "06/14 13:31:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [7][25/25]    t/iou: 0.9090  data_time: 1.7290  time: 2.0873\n",
      "06/14 13:31:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][10/92]  lr: 2.0000e-04  eta: 0:12:25  time: 0.6430  data_time: 0.1299  memory: 6874  loss: 0.1874\n",
      "06/14 13:31:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][20/92]  lr: 2.0000e-04  eta: 0:12:19  time: 0.6434  data_time: 0.1292  memory: 6874  loss: 0.1869\n",
      "06/14 13:31:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][30/92]  lr: 2.0000e-04  eta: 0:12:12  time: 0.6440  data_time: 0.1285  memory: 6874  loss: 0.1881\n",
      "06/14 13:31:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][40/92]  lr: 2.0000e-04  eta: 0:12:06  time: 0.6448  data_time: 0.1278  memory: 6874  loss: 0.1861\n",
      "06/14 13:32:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][50/92]  lr: 2.0000e-04  eta: 0:12:01  time: 0.6452  data_time: 0.1272  memory: 6874  loss: 0.1811\n",
      "06/14 13:32:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][60/92]  lr: 2.0000e-04  eta: 0:11:55  time: 0.6456  data_time: 0.1273  memory: 6874  loss: 0.1771\n",
      "06/14 13:32:15 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][70/92]  lr: 2.0000e-04  eta: 0:11:49  time: 0.6453  data_time: 0.1273  memory: 6874  loss: 0.1730\n",
      "06/14 13:32:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][80/92]  lr: 2.0000e-04  eta: 0:11:43  time: 0.6461  data_time: 0.1279  memory: 6874  loss: 0.1716\n",
      "06/14 13:32:29 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [8][90/92]  lr: 2.0000e-04  eta: 0:11:37  time: 0.6476  data_time: 0.1279  memory: 6874  loss: 0.1715\n",
      "06/14 13:32:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:33:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [8][10/25]    eta: 0:01:10  time: 2.1595  data_time: 1.8010  memory: 6874  \n",
      "06/14 13:33:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [8][20/25]    eta: 0:00:12  time: 2.1601  data_time: 1.8017  memory: 1696  \n",
      "06/14 13:33:24 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [8][25/25]    t/iou: 0.9137  data_time: 1.7477  time: 2.1062\n",
      "06/14 13:33:31 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][10/92]  lr: 2.0000e-04  eta: 0:11:30  time: 0.6461  data_time: 0.1287  memory: 6874  loss: 0.1711\n",
      "06/14 13:33:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][20/92]  lr: 2.0000e-04  eta: 0:11:23  time: 0.6471  data_time: 0.1287  memory: 6874  loss: 0.1695\n",
      "06/14 13:33:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][30/92]  lr: 2.0000e-04  eta: 0:11:17  time: 0.6482  data_time: 0.1285  memory: 6874  loss: 0.1729\n",
      "06/14 13:33:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][40/92]  lr: 2.0000e-04  eta: 0:11:11  time: 0.6486  data_time: 0.1286  memory: 6874  loss: 0.1710\n",
      "06/14 13:33:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][50/92]  lr: 2.0000e-04  eta: 0:11:05  time: 0.6493  data_time: 0.1286  memory: 6874  loss: 0.1727\n",
      "06/14 13:34:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][60/92]  lr: 2.0000e-04  eta: 0:10:59  time: 0.6492  data_time: 0.1285  memory: 6874  loss: 0.1796\n",
      "06/14 13:34:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][70/92]  lr: 2.0000e-04  eta: 0:10:53  time: 0.6491  data_time: 0.1283  memory: 6874  loss: 0.1825\n",
      "06/14 13:34:16 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][80/92]  lr: 2.0000e-04  eta: 0:10:47  time: 0.6480  data_time: 0.1279  memory: 6874  loss: 0.1866\n",
      "06/14 13:34:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [9][90/92]  lr: 2.0000e-04  eta: 0:10:41  time: 0.6471  data_time: 0.1274  memory: 6874  loss: 0.1931\n",
      "06/14 13:34:24 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:35:10 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [9][10/25]    eta: 0:01:10  time: 2.1599  data_time: 1.8018  memory: 6874  \n",
      "06/14 13:35:15 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val)  [9][20/25]    eta: 0:00:12  time: 2.1595  data_time: 1.8015  memory: 1696  \n",
      "06/14 13:35:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [9][25/25]    t/iou: 0.8954  data_time: 1.7373  time: 2.0921\n",
      "06/14 13:35:24 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][10/92]  lr: 2.0000e-04  eta: 0:10:33  time: 0.6420  data_time: 0.1266  memory: 6874  loss: 0.1934\n",
      "06/14 13:35:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][20/92]  lr: 2.0000e-04  eta: 0:10:27  time: 0.6417  data_time: 0.1261  memory: 6874  loss: 0.1948\n",
      "06/14 13:35:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][30/92]  lr: 2.0000e-04  eta: 0:10:20  time: 0.6423  data_time: 0.1264  memory: 6874  loss: 0.1967\n",
      "06/14 13:35:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][40/92]  lr: 2.0000e-04  eta: 0:10:14  time: 0.6425  data_time: 0.1265  memory: 6874  loss: 0.1970\n",
      "06/14 13:35:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][50/92]  lr: 2.0000e-04  eta: 0:10:08  time: 0.6425  data_time: 0.1263  memory: 6874  loss: 0.1992\n",
      "06/14 13:35:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][60/92]  lr: 2.0000e-04  eta: 0:10:02  time: 0.6438  data_time: 0.1263  memory: 6874  loss: 0.1975\n",
      "06/14 13:36:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][70/92]  lr: 2.0000e-04  eta: 0:09:56  time: 0.6430  data_time: 0.1262  memory: 6874  loss: 0.1941\n",
      "06/14 13:36:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][80/92]  lr: 2.0000e-04  eta: 0:09:50  time: 0.6431  data_time: 0.1264  memory: 6874  loss: 0.1917\n",
      "06/14 13:36:15 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [10][90/92]  lr: 2.0000e-04  eta: 0:09:44  time: 0.6429  data_time: 0.1264  memory: 6874  loss: 0.1892\n",
      "06/14 13:36:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:36:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n",
      "06/14 13:37:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][10/25]    eta: 0:01:09  time: 2.1577  data_time: 1.8007  memory: 6874  \n",
      "06/14 13:37:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][20/25]    eta: 0:00:12  time: 2.1568  data_time: 1.8000  memory: 1696  \n",
      "06/14 13:37:11 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][25/25]    t/iou: 0.9114  data_time: 1.7278  time: 2.0819\n",
      "06/14 13:37:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][10/92]  lr: 2.0000e-04  eta: 0:09:36  time: 0.6408  data_time: 0.1265  memory: 6874  loss: 0.1840\n",
      "06/14 13:37:23 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][20/92]  lr: 2.0000e-04  eta: 0:09:29  time: 0.6407  data_time: 0.1265  memory: 6874  loss: 0.1837\n",
      "06/14 13:37:30 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][30/92]  lr: 2.0000e-04  eta: 0:09:23  time: 0.6404  data_time: 0.1263  memory: 6874  loss: 0.1818\n",
      "06/14 13:37:36 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][40/92]  lr: 2.0000e-04  eta: 0:09:17  time: 0.6404  data_time: 0.1260  memory: 6874  loss: 0.1779\n",
      "06/14 13:37:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][50/92]  lr: 2.0000e-04  eta: 0:09:10  time: 0.6402  data_time: 0.1266  memory: 6874  loss: 0.1742\n",
      "06/14 13:37:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][60/92]  lr: 2.0000e-04  eta: 0:09:04  time: 0.6413  data_time: 0.1273  memory: 6874  loss: 0.1686\n",
      "06/14 13:37:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][70/92]  lr: 2.0000e-04  eta: 0:08:58  time: 0.6411  data_time: 0.1280  memory: 6874  loss: 0.1688\n",
      "06/14 13:38:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:38:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][80/92]  lr: 2.0000e-04  eta: 0:08:52  time: 0.6407  data_time: 0.1282  memory: 6874  loss: 0.1628\n",
      "06/14 13:38:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [11][90/92]  lr: 2.0000e-04  eta: 0:08:46  time: 0.6406  data_time: 0.1282  memory: 6874  loss: 0.1598\n",
      "06/14 13:38:10 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:38:57 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [11][10/25]    eta: 0:01:09  time: 2.1567  data_time: 1.8000  memory: 6874  \n",
      "06/14 13:39:01 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [11][20/25]    eta: 0:00:12  time: 2.1566  data_time: 1.7999  memory: 1696  \n",
      "06/14 13:39:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [11][25/25]    t/iou: 0.9188  data_time: 1.7296  time: 2.0887\n",
      "06/14 13:39:10 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][10/92]  lr: 2.0000e-04  eta: 0:08:38  time: 0.6384  data_time: 0.1284  memory: 6874  loss: 0.1537\n",
      "06/14 13:39:16 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][20/92]  lr: 2.0000e-04  eta: 0:08:32  time: 0.6387  data_time: 0.1283  memory: 6874  loss: 0.1523\n",
      "06/14 13:39:23 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][30/92]  lr: 2.0000e-04  eta: 0:08:25  time: 0.6399  data_time: 0.1285  memory: 6874  loss: 0.1488\n",
      "06/14 13:39:29 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][40/92]  lr: 2.0000e-04  eta: 0:08:19  time: 0.6414  data_time: 0.1287  memory: 6874  loss: 0.1475\n",
      "06/14 13:39:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][50/92]  lr: 2.0000e-04  eta: 0:08:13  time: 0.6422  data_time: 0.1288  memory: 6874  loss: 0.1454\n",
      "06/14 13:39:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][60/92]  lr: 2.0000e-04  eta: 0:08:07  time: 0.6426  data_time: 0.1281  memory: 6874  loss: 0.1439\n",
      "06/14 13:39:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][70/92]  lr: 2.0000e-04  eta: 0:08:00  time: 0.6424  data_time: 0.1273  memory: 6874  loss: 0.1432\n",
      "06/14 13:39:55 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][80/92]  lr: 2.0000e-04  eta: 0:07:54  time: 0.6420  data_time: 0.1267  memory: 6874  loss: 0.1393\n",
      "06/14 13:40:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [12][90/92]  lr: 2.0000e-04  eta: 0:07:48  time: 0.6424  data_time: 0.1275  memory: 6874  loss: 0.1383\n",
      "06/14 13:40:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:40:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [12][10/25]    eta: 0:01:09  time: 2.1518  data_time: 1.7951  memory: 6874  \n",
      "06/14 13:40:54 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [12][20/25]    eta: 0:00:12  time: 2.1510  data_time: 1.7945  memory: 1696  \n",
      "06/14 13:40:57 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [12][25/25]    t/iou: 0.9193  data_time: 1.7258  time: 2.0839\n",
      "06/14 13:41:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][10/92]  lr: 2.0000e-04  eta: 0:07:40  time: 0.6407  data_time: 0.1276  memory: 6874  loss: 0.1361\n",
      "06/14 13:41:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][20/92]  lr: 2.0000e-04  eta: 0:07:34  time: 0.6411  data_time: 0.1276  memory: 6874  loss: 0.1371\n",
      "06/14 13:41:15 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][30/92]  lr: 2.0000e-04  eta: 0:07:28  time: 0.6421  data_time: 0.1275  memory: 6874  loss: 0.1368\n",
      "06/14 13:41:22 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][40/92]  lr: 2.0000e-04  eta: 0:07:21  time: 0.6434  data_time: 0.1274  memory: 6874  loss: 0.1362\n",
      "06/14 13:41:28 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][50/92]  lr: 2.0000e-04  eta: 0:07:15  time: 0.6436  data_time: 0.1272  memory: 6874  loss: 0.1351\n",
      "06/14 13:41:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][60/92]  lr: 2.0000e-04  eta: 0:07:09  time: 0.6443  data_time: 0.1271  memory: 6874  loss: 0.1341\n",
      "06/14 13:41:41 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][70/92]  lr: 2.0000e-04  eta: 0:07:02  time: 0.6438  data_time: 0.1270  memory: 6874  loss: 0.1323\n",
      "06/14 13:41:48 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][80/92]  lr: 2.0000e-04  eta: 0:06:56  time: 0.6455  data_time: 0.1277  memory: 6874  loss: 0.1313\n",
      "06/14 13:41:55 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [13][90/92]  lr: 2.0000e-04  eta: 0:06:50  time: 0.6450  data_time: 0.1278  memory: 6874  loss: 0.1308\n",
      "06/14 13:41:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:42:43 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [13][10/25]    eta: 0:01:09  time: 2.1507  data_time: 1.7934  memory: 6874  \n",
      "06/14 13:42:47 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [13][20/25]    eta: 0:00:12  time: 2.1512  data_time: 1.7933  memory: 1696  \n",
      "06/14 13:42:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [13][25/25]    t/iou: 0.9200  data_time: 1.7338  time: 2.0937\n",
      "06/14 13:42:56 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][10/92]  lr: 2.0000e-04  eta: 0:06:42  time: 0.6410  data_time: 0.1270  memory: 6874  loss: 0.1303\n",
      "06/14 13:43:02 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][20/92]  lr: 2.0000e-04  eta: 0:06:36  time: 0.6411  data_time: 0.1270  memory: 6874  loss: 0.1293\n",
      "06/14 13:43:09 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][30/92]  lr: 2.0000e-04  eta: 0:06:29  time: 0.6417  data_time: 0.1272  memory: 6874  loss: 0.1262\n",
      "06/14 13:43:15 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][40/92]  lr: 2.0000e-04  eta: 0:06:23  time: 0.6422  data_time: 0.1273  memory: 6874  loss: 0.1256\n",
      "06/14 13:43:21 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][50/92]  lr: 2.0000e-04  eta: 0:06:17  time: 0.6431  data_time: 0.1276  memory: 6874  loss: 0.1246\n",
      "06/14 13:43:28 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][60/92]  lr: 2.0000e-04  eta: 0:06:11  time: 0.6435  data_time: 0.1278  memory: 6874  loss: 0.1241\n",
      "06/14 13:43:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][70/92]  lr: 2.0000e-04  eta: 0:06:04  time: 0.6435  data_time: 0.1280  memory: 6874  loss: 0.1225\n",
      "06/14 13:43:41 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][80/92]  lr: 2.0000e-04  eta: 0:05:58  time: 0.6431  data_time: 0.1280  memory: 6874  loss: 0.1227\n",
      "06/14 13:43:48 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [14][90/92]  lr: 2.0000e-04  eta: 0:05:52  time: 0.6419  data_time: 0.1278  memory: 6874  loss: 0.1220\n",
      "06/14 13:43:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:44:35 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [14][10/25]    eta: 0:01:09  time: 2.1508  data_time: 1.7923  memory: 6874  \n",
      "06/14 13:44:40 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [14][20/25]    eta: 0:00:12  time: 2.1507  data_time: 1.7922  memory: 1696  \n",
      "06/14 13:44:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [14][25/25]    t/iou: 0.9211  data_time: 1.7234  time: 2.0814\n",
      "06/14 13:44:49 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][10/92]  lr: 2.0000e-04  eta: 0:05:44  time: 0.6390  data_time: 0.1274  memory: 6874  loss: 0.1238\n",
      "06/14 13:44:55 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][20/92]  lr: 2.0000e-04  eta: 0:05:38  time: 0.6392  data_time: 0.1273  memory: 6874  loss: 0.1232\n",
      "06/14 13:45:01 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][30/92]  lr: 2.0000e-04  eta: 0:05:31  time: 0.6393  data_time: 0.1272  memory: 6874  loss: 0.1234\n",
      "06/14 13:45:07 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][40/92]  lr: 2.0000e-04  eta: 0:05:25  time: 0.6393  data_time: 0.1271  memory: 6874  loss: 0.1231\n",
      "06/14 13:45:14 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][50/92]  lr: 2.0000e-04  eta: 0:05:19  time: 0.6385  data_time: 0.1269  memory: 6874  loss: 0.1223\n",
      "06/14 13:45:20 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][60/92]  lr: 2.0000e-04  eta: 0:05:12  time: 0.6391  data_time: 0.1267  memory: 6874  loss: 0.1231\n",
      "06/14 13:45:27 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][70/92]  lr: 2.0000e-04  eta: 0:05:06  time: 0.6378  data_time: 0.1265  memory: 6874  loss: 0.1214\n",
      "06/14 13:45:33 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][80/92]  lr: 2.0000e-04  eta: 0:05:00  time: 0.6368  data_time: 0.1263  memory: 6874  loss: 0.1205\n",
      "06/14 13:45:40 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [15][90/92]  lr: 2.0000e-04  eta: 0:04:53  time: 0.6358  data_time: 0.1264  memory: 6874  loss: 0.1183\n",
      "06/14 13:45:41 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:46:27 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [15][10/25]    eta: 0:01:09  time: 2.1485  data_time: 1.7899  memory: 6874  \n",
      "06/14 13:46:32 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [15][20/25]    eta: 0:00:12  time: 2.1486  data_time: 1.7898  memory: 1696  \n",
      "06/14 13:46:34 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [15][25/25]    t/iou: 0.9198  data_time: 1.7204  time: 2.0790\n",
      "06/14 13:46:41 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][10/92]  lr: 2.0000e-04  eta: 0:04:46  time: 0.6338  data_time: 0.1258  memory: 6874  loss: 0.1188\n",
      "06/14 13:46:47 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][20/92]  lr: 2.0000e-04  eta: 0:04:39  time: 0.6342  data_time: 0.1258  memory: 6874  loss: 0.1174\n",
      "06/14 13:46:53 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][30/92]  lr: 2.0000e-04  eta: 0:04:33  time: 0.6354  data_time: 0.1258  memory: 6874  loss: 0.1167\n",
      "06/14 13:46:59 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][40/92]  lr: 2.0000e-04  eta: 0:04:26  time: 0.6364  data_time: 0.1259  memory: 6874  loss: 0.1156\n",
      "06/14 13:47:06 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][50/92]  lr: 2.0000e-04  eta: 0:04:20  time: 0.6381  data_time: 0.1267  memory: 6874  loss: 0.1161\n",
      "06/14 13:47:13 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][60/92]  lr: 2.0000e-04  eta: 0:04:14  time: 0.6397  data_time: 0.1273  memory: 6874  loss: 0.1137\n",
      "06/14 13:47:19 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][70/92]  lr: 2.0000e-04  eta: 0:04:07  time: 0.6386  data_time: 0.1272  memory: 6874  loss: 0.1138\n",
      "06/14 13:47:25 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][80/92]  lr: 2.0000e-04  eta: 0:04:01  time: 0.6390  data_time: 0.1271  memory: 6874  loss: 0.1135\n",
      "06/14 13:47:32 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [16][90/92]  lr: 2.0000e-04  eta: 0:03:55  time: 0.6394  data_time: 0.1270  memory: 6874  loss: 0.1139\n",
      "06/14 13:47:33 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240614_131828\n",
      "06/14 13:48:20 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [16][10/25]    eta: 0:01:09  time: 2.1494  data_time: 1.7906  memory: 6874  \n",
      "06/14 13:48:25 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [16][20/25]    eta: 0:00:12  time: 2.1494  data_time: 1.7904  memory: 1696  \n",
      "06/14 13:48:27 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [16][25/25]    t/iou: 0.9221  data_time: 1.7292  time: 2.0899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMedLvmWorkflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainClass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainTool\n\u001b[1;32m      2\u001b[0m trainProcess\u001b[38;5;241m=\u001b[39mTrainTool()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolor_to_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_to_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trainlogs/train_deeplabv3_resnet50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/trainClass/train.py:23\u001b[0m, in \u001b[0;36mTrainTool.startTrain\u001b[0;34m(self, model, trainDataSet, valDataSet, testDataSet, batch_size, max_epochs, val_interval, checkpoint_interval, work_dir, vis_backends, color_to_class)\u001b[0m\n\u001b[1;32m     21\u001b[0m visualizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisualizer\u001b[39m\u001b[38;5;124m'\u001b[39m, vis_backends\u001b[38;5;241m=\u001b[39mvis_backends)\n\u001b[1;32m     22\u001b[0m trainC\u001b[38;5;241m=\u001b[39mTrainBase()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrainDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtestDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                         \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mval_evaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcustom_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdefault_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualizer\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/trainClass/baseTool/trainBase.py:48\u001b[0m, in \u001b[0;36mTrainBase.train\u001b[0;34m(self, model, trainDataSet, valDataSet, testDataSet, dataSetShuffle, batch_size, work_dir, optim_wrapper, train_cfg, val_cfg, val_evaluator, test_evaluator, test_cfg, visualizer, custom_hooks, default_hooks)\u001b[0m\n\u001b[1;32m     29\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     work_dir\u001b[38;5;241m=\u001b[39mwork_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     default_hooks\u001b[38;5;241m=\u001b[39mdefault_hooks\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m runner\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedLvmWorkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/runner.py:1777\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_compile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1777\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:96\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mval_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_begin\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    102\u001b[0m                  \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs)):\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:113\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:129\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[0;34m(self, idx, data_batch)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_idx\u001b[38;5;241m=\u001b[39midx, data_batch\u001b[38;5;241m=\u001b[39mdata_batch)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    134\u001b[0m     batch_idx\u001b[38;5;241m=\u001b[39midx,\n\u001b[1;32m    135\u001b[0m     data_batch\u001b[38;5;241m=\u001b[39mdata_batch,\n\u001b[1;32m    136\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/model/base_model/base_model.py:116\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[1;32m    114\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_forward(data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    115\u001b[0m parsed_losses, log_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_losses(losses)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_vars\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/optim/optimizer/optimizer_wrapper.py:201\u001b[0m, in \u001b[0;36mOptimWrapper.update_params\u001b[0;34m(self, loss, step_kwargs, zero_kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Update parameters only if `self._inner_count` is divisible by\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# `self._accumulative_counts` or `self._inner_count` equals to\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# `self._max_counts`\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_update():\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/optim/optimizer/amp_optimizer_wrapper.py:139\u001b[0m, in \u001b[0;36mAmpOptimWrapper.step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_scaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_grad()\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_scaler\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_update_param)\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from MedLvmWorkflow.trainClass.train import TrainTool\n",
    "trainProcess=TrainTool()\n",
    "trainProcess.startTrain(trainModel,\n",
    "                        train_set,\n",
    "                        valid_set,\n",
    "                        batch_size=4,\n",
    "                        max_epochs=20,\n",
    "                        checkpoint_interval=10,\n",
    "                        color_to_class=color_to_class,work_dir='./trainlogs/train_deeplabv3_resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m color_to_class\u001b[38;5;241m=\u001b[39mcreate_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/CamVid/class_dict.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(color_to_class\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m----> 8\u001b[0m inferModel \u001b[38;5;241m=\u001b[39m\u001b[43mcreateInferModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeeplabv3_resnet50_pretrained\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbackboneModlePertrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./epoch_20.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(inferModel)\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/models/modelFactory.py:45\u001b[0m, in \u001b[0;36mcreateInferModel\u001b[0;34m(backboneModle, num_classes, headModel, backboneModlePertrained, headModlePertrained)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m createCusInferModel(modesConfig[backboneModle],num_classes,headModel\u001b[38;5;241m=\u001b[39mmodesConfig[headModel],backboneModlePertrained\u001b[38;5;241m=\u001b[39mbackboneModlePertrained,headModlePertrained\u001b[38;5;241m=\u001b[39mheadModlePertrained)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreateCusInferModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodesConfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackboneModle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbackboneModlePertrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackboneModlePertrained\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/models/modelFactory.py:60\u001b[0m, in \u001b[0;36mcreateCusInferModel\u001b[0;34m(backboneModle, num_classes, headModel, backboneModlePertrained, headModlePertrained)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     mModel\u001b[38;5;241m=\u001b[39m \u001b[43mbackboneModle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43muser_to_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     state_dict\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mload(backboneModlePertrained)\n\u001b[1;32m     62\u001b[0m     mModel\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from MedLvmWorkflow.dataset.segmentation.demoDataset import create_palette\n",
    "from MedLvmWorkflow.models.modelFactory import createTrainModel,createInferModel\n",
    "color_to_class=create_palette(\"../data/CamVid/class_dict.csv\")\n",
    "num_class=len(color_to_class.items())\n",
    "inferModel =createInferModel(\"deeplabv3_resnet50_pretrained\",num_class,backboneModlePertrained=\"./epoch_20.pth\")\n",
    "\n",
    "print(inferModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练dinov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/botao/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/botao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/botao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/botao/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 17:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.9.0 (default, Nov 15 2020, 14:28:56) [GCC 7.3.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 34359312\n",
      "    GPU 0: NVIDIA RTX A4000\n",
      "    GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "    CUDA_HOME: /usr/local/cuda-11.6\n",
      "    NVCC: Cuda compilation tools, release 11.6, V11.6.55\n",
      "    GCC: gcc (Ubuntu 7.5.0-6ubuntu2) 7.5.0\n",
      "    PyTorch: 2.1.2+cu118\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.16.2+cu118\n",
      "    OpenCV: 4.9.0\n",
      "    MMEngine: 0.10.4\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 34359312\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/13 17:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/13 17:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) SegmentationVisHook                \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "06/13 17:31:04 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CamVid has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "06/13 17:31:04 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CamVid has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "06/13 17:31:04 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/13 17:31:04 - MedLvmWorkflow - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "06/13 17:31:04 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/botao/liangjun/LVMWorkFlow/notebooks/trainlogs/train_dinov2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botao/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 17:31:08 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][10/46]  lr: 7.0000e-04  eta: 0:01:39  time: 0.4504  data_time: 0.2548  memory: 505  loss: 2.8780\n",
      "06/13 17:31:11 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][20/46]  lr: 7.0000e-04  eta: 0:01:19  time: 0.3774  data_time: 0.2468  memory: 505  loss: 2.7731\n",
      "06/13 17:31:14 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][30/46]  lr: 7.0000e-04  eta: 0:01:10  time: 0.3520  data_time: 0.2431  memory: 505  loss: 2.7331\n",
      "06/13 17:31:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][40/46]  lr: 7.0000e-04  eta: 0:01:04  time: 0.3392  data_time: 0.2412  memory: 505  loss: 2.7142\n",
      "06/13 17:31:19 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240613_173102\n",
      "06/13 17:31:19 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botao/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 17:31:27 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][10/12]    eta: 0:00:01  time: 0.8032  data_time: 0.6982  memory: 571  \n",
      "06/13 17:31:28 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][12/12]    t/iou: 0.5451  data_time: 0.6211  time: 0.7260\n",
      "06/13 17:31:31 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][10/46]  lr: 7.0000e-04  eta: 0:00:56  time: 0.3271  data_time: 0.2385  memory: 571  loss: 2.6930\n",
      "06/13 17:31:34 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][20/46]  lr: 7.0000e-04  eta: 0:00:53  time: 0.3250  data_time: 0.2399  memory: 505  loss: 2.6846\n",
      "06/13 17:31:37 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][30/46]  lr: 7.0000e-04  eta: 0:00:49  time: 0.3216  data_time: 0.2390  memory: 505  loss: 2.6795\n",
      "06/13 17:31:40 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][40/46]  lr: 7.0000e-04  eta: 0:00:45  time: 0.3188  data_time: 0.2382  memory: 505  loss: 2.6744\n",
      "06/13 17:31:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240613_173102\n",
      "06/13 17:31:42 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botao/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 17:31:50 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][10/12]    eta: 0:00:01  time: 0.7612  data_time: 0.6559  memory: 571  \n",
      "06/13 17:31:51 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][12/12]    t/iou: 0.5412  data_time: 0.5910  time: 0.6967\n",
      "06/13 17:31:54 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][10/46]  lr: 7.0000e-04  eta: 0:00:40  time: 0.3024  data_time: 0.2370  memory: 571  loss: 2.6557\n",
      "06/13 17:31:57 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][20/46]  lr: 7.0000e-04  eta: 0:00:37  time: 0.3012  data_time: 0.2358  memory: 505  loss: 2.6427\n",
      "06/13 17:32:00 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][30/46]  lr: 7.0000e-04  eta: 0:00:33  time: 0.3008  data_time: 0.2354  memory: 505  loss: 2.6390\n",
      "06/13 17:32:03 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][40/46]  lr: 7.0000e-04  eta: 0:00:30  time: 0.3015  data_time: 0.2361  memory: 505  loss: 2.6366\n",
      "06/13 17:32:05 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240613_173102\n",
      "06/13 17:32:05 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botao/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/13 17:32:13 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][10/12]    eta: 0:00:01  time: 0.7497  data_time: 0.6443  memory: 571  \n",
      "06/13 17:32:14 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][12/12]    t/iou: 0.5503  data_time: 0.5932  time: 0.6985\n",
      "06/13 17:32:17 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][10/46]  lr: 7.0000e-04  eta: 0:00:25  time: 0.3013  data_time: 0.2359  memory: 571  loss: 2.6347\n",
      "06/13 17:32:20 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][20/46]  lr: 7.0000e-04  eta: 0:00:22  time: 0.3009  data_time: 0.2355  memory: 505  loss: 2.6330\n",
      "06/13 17:32:23 - MedLvmWorkflow - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][30/46]  lr: 7.0000e-04  eta: 0:00:19  time: 0.2999  data_time: 0.2345  memory: 505  loss: 2.6322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m valid_set \u001b[38;5;241m=\u001b[39m CamVid(\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/CamVid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     img_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     mask_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_labels\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtrainModel\u001b[38;5;241m.\u001b[39mtransform_img,\n\u001b[1;32m     17\u001b[0m     target_transform\u001b[38;5;241m=\u001b[39mtrainModel\u001b[38;5;241m.\u001b[39mtransform_lable)\n\u001b[1;32m     18\u001b[0m trainModel\u001b[38;5;241m.\u001b[39moptim[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0007\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolor_to_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_to_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./trainlogs/train_dinov2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/trainClass/train.py:23\u001b[0m, in \u001b[0;36mTrainTool.startTrain\u001b[0;34m(self, model, trainDataSet, valDataSet, testDataSet, batch_size, max_epochs, val_interval, checkpoint_interval, work_dir, vis_backends, color_to_class)\u001b[0m\n\u001b[1;32m     21\u001b[0m visualizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisualizer\u001b[39m\u001b[38;5;124m'\u001b[39m, vis_backends\u001b[38;5;241m=\u001b[39mvis_backends)\n\u001b[1;32m     22\u001b[0m trainC\u001b[38;5;241m=\u001b[39mTrainBase()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrainDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtestDataSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                         \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mval_evaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcustom_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdefault_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_hooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualizer\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/liangjun/LVMWorkFlow/MedLvmWorkflow/trainClass/baseTool/trainBase.py:48\u001b[0m, in \u001b[0;36mTrainBase.train\u001b[0;34m(self, model, trainDataSet, valDataSet, testDataSet, dataSetShuffle, batch_size, work_dir, optim_wrapper, train_cfg, val_cfg, val_evaluator, test_evaluator, test_cfg, visualizer, custom_hooks, default_hooks)\u001b[0m\n\u001b[1;32m     29\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     31\u001b[0m     work_dir\u001b[38;5;241m=\u001b[39mwork_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     default_hooks\u001b[38;5;241m=\u001b[39mdefault_hooks\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m runner\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMedLvmWorkflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/runner.py:1777\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;66;03m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;66;03m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_compile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1777\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:96\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decide_current_val_interval()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mval_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_begin\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    102\u001b[0m                  \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs)):\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:113\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/runner/loops.py:129\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[0;34m(self, idx, data_batch)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_idx\u001b[38;5;241m=\u001b[39midx, data_batch\u001b[38;5;241m=\u001b[39mdata_batch)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    134\u001b[0m     batch_idx\u001b[38;5;241m=\u001b[39midx,\n\u001b[1;32m    135\u001b[0m     data_batch\u001b[38;5;241m=\u001b[39mdata_batch,\n\u001b[1;32m    136\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/model/base_model/base_model.py:116\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[0;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[1;32m    114\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_forward(data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    115\u001b[0m parsed_losses, log_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_losses(losses)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_vars\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/optim/optimizer/optimizer_wrapper.py:201\u001b[0m, in \u001b[0;36mOptimWrapper.update_params\u001b[0;34m(self, loss, step_kwargs, zero_kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Update parameters only if `self._inner_count` is divisible by\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# `self._accumulative_counts` or `self._inner_count` equals to\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# `self._max_counts`\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_update():\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/mmengine/optim/optimizer/amp_optimizer_wrapper.py:139\u001b[0m, in \u001b[0;36mAmpOptimWrapper.step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_scaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_grad()\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_scaler\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_update_param)\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/LVMWorkFlow/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from MedLvmWorkflow.trainClass.train import TrainTool\n",
    "from MedLvmWorkflow.models.modelFactory import createTrainModel\n",
    "trainProcess=TrainTool()\n",
    "trainModel =createTrainModel(\"dinov2_s_pretrained\",num_class,\"conv_s\")\n",
    "train_set = CamVid(\n",
    "    '../data/CamVid',\n",
    "    img_folder='train',\n",
    "    mask_folder='train_labels',\n",
    "    transform=trainModel.transform_img,\n",
    "    target_transform=trainModel.transform_lable)\n",
    "\n",
    "valid_set = CamVid(\n",
    "    '../data/CamVid',\n",
    "    img_folder='val',\n",
    "    mask_folder='val_labels',\n",
    "    transform=trainModel.transform_img,\n",
    "    target_transform=trainModel.transform_lable)\n",
    "trainModel.optim[\"lr\"]=0.0007\n",
    "trainProcess.startTrain(trainModel,\n",
    "                        train_set,\n",
    "                        valid_set,\n",
    "                        batch_size=8,\n",
    "                        color_to_class=color_to_class,work_dir='./trainlogs/train_dinov2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LVMWorkFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
